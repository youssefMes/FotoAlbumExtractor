\chapter{Auswertung}

\section{Metriken zur Auswertung}
\subsection{Bildgröße}
Da es in dem Projekt um das Ausschneiden von Bildern geht ist die Bildgröße eine nahe liegende Metrik um die Genauigkeit unserer Software zu messen. Die Größe wird berechnet indem die Höhe mit der Breite multipliziert wird. Von der so ermittelten Größe des ausgeschnittenen Bildes wird die Größe des Ground Truth Bildes abgezogen. Aus dieser Differenz wird der Prozentuale unterschied zum Ground Truth Bild berechnet. Sollte das Ergebnis negativ sein ist das ausgeschnittene Bild kleiner als der Ground Truth.
   
\subsection{Bildmerkmale}
Eine weitere Metrik ist das erkennen gleicher Bildmerkmale. Dazu wird der patent freie ORB (Orientatet FAST and Rotated BRIEF) detektor von OpenCV verwendet der Rotations invariant ist. ORB ist eine fusion zwischen dem FAST Schlüsselpunkt Detektor and BRIEF Beschreibung der Schlüsselpunkte mit vielen Modifikationen um die Geschwindigkeit des Algorithmus zu verbessern (\cite{OpenCVORB}). Die so erkannten Merkmale werden dann der reihe nach anhand der Distanz zum Bildzentrum verglichen. Es wird für jedes erkannte Bildmerkmal die Distanz zum Zentrum des dazu gehörigen Bildes berechnet und das dann mit den Ergebnissen des Ground Truth verglichen. Solange zwischen den beiden Merkmalen keine zu großer Unterschied in der Distanz ist werden sie als gleich angesehen. Durch dieses Verfahren kann erkannt werden das sich der Ausschnitt des ausgeschnittenen Bildes an der selben stelle befindet, wie der Ground Truth und nicht nur die gleiche Größe ausgeschnitten wurde.

\subsection{Struktureller Ähnlichkeit (SSIM)}
Um ein Bild auf Gleichheit zu untersuchen verwenden wir den Strukturellen Ähnlichkeitsindex (Structural Similarity index, SSIM). Dieser berechnet die Ähnlichkeit zwischen verschiedenen Bildausschnitten mit folgender Gleichung:
$$
SSIM(x,y) = \frac{(2 \mu_x \mu_y + c_1)(\sigma_{xy} + c_2)}{(\mu^2_x + \mu^2_y + c_1)(\sigma^2_x + \sigma^2_y + c_2)}
$$ 
mit $\mu$ als Durchschnitt und $\sigma$ als Varianz bzw. Kovarianzmatrix des Bildausschnittes. $c$ sind da um die Division zu stabilisieren (\cite{Wang2004}). Das Verfahren lässt sich nur auf gleich große Bilder anwenden. Aus diesem Grund verkleinern wir die Bilder mit einer linearen Interpolation auf eine Größe von 64 x 64 Pixeln. Auf die so verkleinerten Bilder wird dann der SSIM angewandt. Um so näher das Ergebnis an 1 ist um so ähnlicher sind die beiden Bilder. 

\section{Ausgeschnittene Bilder}
In der Tabelle \ref{tab:metrics} sind die Ergebnisse für einige der Testbilder dargestellte den direkten Vergleich kann man in Abbildung ?? sehen dort sind die Bilder nebeneinander aufgezeigt. 

\begin{table}[h]
	\centering
	\begin{tabular}{c|ccc}
	Bild & Größe & Merkmale & SSIM \\ \hline 
	01 & 1.92 & 3.64 & 0.82 \\
	02 & 1.60 & 2.39 & 0.77 \\
	03 & -5.52 & 10.12 & 0.36 \\
	04 & 3.15 & 16.54 & 0.68 \\ 
	05 & 1.48 & 2.65 & 0.81 \\ \hline
	06 & -4.42 & 23.58 & 0.55 \\ 
	07 & 2.96 & 4.40 & 0.80 \\
	08 & 8.82 & 8.07 & 0.39 \\
	09 & -1.24 & 2.06 & 0.70 \\
	10 & -0.58 & 5.57 & 0.76 \\ \hline
	11 & -4.35 & 28.46 & 0.49 \\
	12 & 2.96 & 1.74 & 0.80 \\
	13 & 1.26 & 3.24 & 0.88 \\
	14 & 3.01 & 1.94 & 0.72 \\
	15 & 0.76 & 19.51 & 0.60 
	\end{tabular}
	\caption{Ergebnisse der Vergleichsmetriken der ausgeschnittenen Bilder.}
	\label{tab:metrics}
\end{table}


\section{Gesichtserkennung}
Die von OpenCV mitgelieferten Methoden zur Gesichtserkennung auf Bildern funktioniert grundsätzlich gut. Sie ist einfach anwendbar, hat eine vertretbare Laufzeit (der Algorithmus läuft nur einige Sekunden auf einer ganzen eingescannten Albumseite) und erkennt zumindest einen Großteil der Gesichter in den Fotos. \\
Um die Performanz der Classifier einzeln und in Kombination auszuwerten, wurden Wahrheitsmatrizen aufgestellt. Ausgewertet werden jeweils die korrekt erkannten Gesichter (True Positives (TP): positive - positive), die nicht erkannten Gesichter (False Negatives (FN): positive - negative) und die fälschlicherweise erkannten Gesichter (False Positives (FP): negative - positive). Korrekt nicht erkannte Gesichter (True Negatives (TN): negative - negative) gibt es in unserem Anwendungsfall nicht.\\
Tabellen \ref{tab:set1frontal} und \ref{tab:set2frontal} zeigen die Metriken in dem ersten und dem zweiten Datensatz wenn nur der frontal faces Classifier angewendet wird. Die daraus errechnete Genauigkeit \footnote{Formel für die Berechnung der Genauigkeit (\grqq Accuracy\grqq) ist: $\frac{TP + TN}{TP + FN + TN + FP}$, ($TN = 0$ in unserem Fall).} beträgt im ersten Datensatz 0.75 und im zweiten 0.59.\\
\begin{table}[h]
	\centering
	\begin{tabular}{l|lll}
		\hline
		& \multicolumn{3}{l}{Reference} \\ \hline
		\multirow{3}{*}{Prediction} &  & Positive & Negative \\
		& Positive & 270 & 1 \\
		& Negative & 88 & - \\ \hline 
	\end{tabular}
	\caption{Wahrheitsmatrix im ersten Datensatz, wenn nur der frontal-Classifier zum Einsatz kommt. Die Genauigkeit der Gesichtserkennung beträgt 0.75.}
	\label{tab:set1frontal}
\end{table}
\begin{table}[h]
	\centering
	\begin{tabular}{llll}
		\hline
		& \multicolumn{3}{l}{Reference} \\ \hline
		\multirow{3}{*}{Prediction} &  & Positive & Negative \\
		& Positive & 282 & 7 \\
		& Negative & 172 & - \\ \cline{2-4} 
	\end{tabular}
	\caption{Wahrheitsmatrix im zweiten Datensatz, wenn nur der frontal-Classifier zum Einsatz kommt. Die Genauigkeit der Gesichtserkennung beträgt 0.59.}
	\label{tab:set2frontal}
\end{table}
Wenn der zweite Classifier zur Erkennung von Profilen mit dazu genommen wird, erhöht sich die Anzahl der erkannten Gesichtern ein wenig. Das ist in Tabellen \ref{tab:set1frontalprofile} und \ref{tab:set2frontalprofile} zu sehen. Die Gesichter, die bereits von dem frontal-Classifier erkannt wurden, werden automatisch heraus gefiltert. Danach bleibt ein kleiner Mehrgewinn durch die Anwendung des profile-Classifiers übrig, die Performanz verbessert sich auf 0.76 und 0.65 jeweils.
\begin{table}[h]
	\centering
	\begin{tabular}{llll}
		\hline
		& \multicolumn{3}{l}{Reference} \\ \hline
		\multirow{3}{*}{Prediction} &  & Positive & Negative \\
		& Positive & 274 & 2 \\
		& Negative & 84 & - \\ \cline{2-4} 
	\end{tabular}
	\caption{Wahrheitsmatrix im ersten Datensatz, wenn frontal- und profile-Classifier kombiniert werden. Die Genauigkeit der Gesichtserkennung beträgt 0.76.}
	\label{tab:set1frontalprofile}
\end{table}
\begin{table}[h]
	\centering
	\begin{tabular}{llll}
		\hline
		& \multicolumn{3}{l}{Reference} \\ \hline
		\multirow{3}{*}{Prediction} &  & Positive & Negative \\
		& Positive & 301 & 8 \\
		& Negative & 153 & - \\ \cline{2-4} 
	\end{tabular}
	\caption{Wahrheitsmatrix im zweiten Datensatz, wenn frontal- und profile-Classifier kombiniert werden. Die Genauigkeit der Gesichtserkennung beträgt 0.65.}
	\label{tab:set2frontalprofile}
\end{table}

\subsubsection*{Festlegung der Ground Truth}
Die Referenz für die Auswertung der Gesichtserkennung festzulegen ist nicht trivial. Auf den alten Fotos sind aufgrund der schlechten Qualität der Aufnahmen viele Gesichter nicht tatsächlich zu erkennen. Vielmehr erschließt sich ein menschlicher Betrachter aus dem Kontext die Information, dass sich an gewissen Stellen ein Gesicht befinden muss. Das wurde bei der Festlegung der Ground Truth berücksichtigt. So wurden beispielsweise besonders kleine, verdeckte oder stark verschwommene Gesichter für die Berechnung der Metriken nicht berücksichtigt. Die Classifier haben außerdem Probleme, Gesichter zu erkennen, wenn starke Okklusionen auftreten, selbst wenn das Gesicht nur im Profil zu erkennen ist oder die Person eine Kopfbedeckung trägt. Diese wiederum wurden in die Gesamtsumme der Gesichter mit einberechnet. Im Folgenden werden exemplarisch einige Bilder gezeigt, die der Gesichtserkennungsalgorithmus nicht erkennen kann und die deswegen nicht in die Auswertung mit einbezogen wurden.\\
Bild \ref{fig:neg_ex1} zeigt ein kleines Mädchen, jedoch ist die Aufnahme sehr verschwommen. Jeder Mensch, der das Bild betrachtet, weiß, dass das Mädchen ein Gesicht haben muss. Wenn jedoch die Kontextinformationen entfernt werden würde, wäre es selbst für einen Menschen schwer, ein Gesicht zu identifizieren.\\
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\linewidth]{images/examples_groundtruth/negative/04_1.png}
	\caption{Starke Bewegung im Bild verhindert, dass das Gesicht des Mädchens tatsächlich zu sehen ist, auch wenn menschliche Betrachter durchaus ihr Gesicht erkennen könnten.}
	\label{fig:neg_ex1}
\end{figure}
Auf Bild \ref{fig:neg_ex2} ist eine ähnliche Situationen zu sehen: alle abgebildeten Menschen haben Gesichter, jedoch sind sie objektiv nicht wirklich zu sehen. Sie sind relativ klein, was normalerweise für den Classifier kein Problem darstellt, aber eine schlechte Beleuchtung und geringer Kontrast tragen dazu bei, dass die Gesichter nicht erkannt werden.\\
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\linewidth]{images/examples_groundtruth/negative/29_2.png}
	\caption{Die Gesichtserkennung schlägt fehlt, wenn die Gesichter zu schlecht beleuchtet sind und/oder zu wenig Kontrast aufweisen. Auch, wenn normalerweise auch Gesichter in dieser Größe von dem Algorithmus problemlos erkannt werden.}
	\label{fig:neg_ex2}
\end{figure}
Um dennoch eine repräsentative Auswertung der Performanz der Gesichtserkennung zu gewährleisten, wurden viele nicht erkannte Gesichter mit in die Metriken einberechnet. Beispielsweise können die Classifier keine Gesichter mit Kopfbedeckungen erkennen. Ansonsten sind aber die Gesichter gut zu identifizieren, was in Bild \ref{fig:pos_ex1} zu sehen ist.\\
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\linewidth]{images/examples_groundtruth/positive/32_3.png}
	\caption{Kein Classifier erkennt Gesichter mit Hüten, auch wenn sie nur einen kleinen Teil vom Gesicht verdecken. Alle anderen Gesichter werden problemlos identifiziert.}
	\label{fig:pos_ex1}
\end{figure}
Bild \ref{fig:pos_ex2} zeigt einen problematischen Grenzfall aus dem zweiten Datensatz. In diesem Bild wurden insgesamt drei Gesichter erkannt (zwei vom frontal-Classifier (grün) und eins vom profile-Classifier (rot)). Da alle Gesichter in etwa die gleiche Größe und Schärfe haben, sollte angenommen werden, dass auch die restlichen Personen erkannt werden. Es ist nicht ersichtlich, warum auf diesem Bild beide Classifier keine weiteren Gesichter erkennen. Von daher fließen alle Gesichter auf diesem Foto mit in die Bewertung ein und erhöhen die Anzahl der False Negatives enorm. Besonders im zweiten Datensatz kommen solche Fälle häufiger vor, was die schlechte Performanz des Algorithmus' im Vergleich zum ersten Datensatz erklärt.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\linewidth]{images/examples_groundtruth/positive/16_1.png}
	\caption{In diesem Gruppenfoto werden nur wenige Gesichter erkannt, obwohl sie alle ähnlich aussehen. Das verschlechtert die Kennwerte für die Genauigkeit besonders im zweiten Datensatz stark.}
	\label{fig:pos_ex2}
\end{figure}